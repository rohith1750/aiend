{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmgZcK9Zn8zJ"
   },
   "source": [
    "Saai Narayann Maddu\n",
    "\n",
    "210968036\n",
    "\n",
    "Section A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UVvypevYmjc9"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVGs4sxdml_2",
    "outputId": "66c9e860-aa63-4f5f-9b49-777243fb9d47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K6f8LJyUmoPX"
   },
   "outputs": [],
   "source": [
    "def monte_carlo_exploring_starts(env, num_episodes=500):\n",
    "    \"\"\"\n",
    "    Performs Monte Carlo with Exploring Starts algorithm to estimate the optimal policy.\n",
    "\n",
    "    Args:\n",
    "    - env: The environment object representing the Markov Decision Process (MDP).\n",
    "    - num_episodes: Number of episodes to run.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_policy: The optimal policy derived from Q values.\n",
    "    - Q_values: The state-action value function.\n",
    "    - total_steps: List containing the number of steps taken in each episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Q values and N counts for each state-action pair\n",
    "    Q_values = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N_counts = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    gamma = 1.0  # discount factor\n",
    "    total_steps = []  # List to store the number of steps taken in each episode\n",
    "\n",
    "    # Iterate over episodes\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()  # Initialize the environment and get the initial state\n",
    "        episode = []  # List to store the sequence of (state, action, reward) tuples for the episode\n",
    "        done = False\n",
    "        steps = 0  # Counter to track the number of steps taken in the episode\n",
    "\n",
    "        # Generate an episode using exploring starts\n",
    "        while not done:\n",
    "            action = np.random.choice(env.action_space.n)  # Choose a random action\n",
    "            next_state, reward, done, info = env.step(action)  # Take the chosen action\n",
    "            episode.append((state, action, reward))  # Record the transition\n",
    "            state = next_state  # Update the current state\n",
    "            steps += 1  # Increment the step counter\n",
    "\n",
    "        total_steps.append(steps)  # Record the total number of steps taken in the episode\n",
    "\n",
    "        # Update Q values using the episode\n",
    "        returns = 0\n",
    "        for j in range(len(episode) - 1, -1, -1):  # Iterate over the episode in reverse order\n",
    "            state, action, reward = episode[j]\n",
    "            returns = gamma * returns + reward  # Compute the return\n",
    "            N_counts[state][action] += 1  # Increment the visit count for the state-action pair\n",
    "            Q_values[state][action] += (returns - Q_values[state][action]) / N_counts[state][action]  # Update Q value\n",
    "\n",
    "    # Derive optimal policy from Q values\n",
    "    optimal_policy = np.argmax(Q_values, axis=1)\n",
    "\n",
    "    return optimal_policy, Q_values, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WEYsH-Icm1U_"
   },
   "outputs": [],
   "source": [
    "def on_policy_monte_carlo_control(env, num_episodes=500, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Performs On-Policy Monte Carlo Control with Ɛ-greedy policy to estimate the optimal policy.\n",
    "\n",
    "    Args:\n",
    "    - env: The environment object representing the Markov Decision Process (MDP).\n",
    "    - num_episodes: Number of episodes to run.\n",
    "    - epsilon: The probability of choosing a random action (exploration).\n",
    "\n",
    "    Returns:\n",
    "    - optimal_policy: The optimal policy derived from Q values.\n",
    "    - Q_values: The state-action value function.\n",
    "    - total_steps: List containing the number of steps taken in each episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Q values and N counts for each state-action pair\n",
    "    Q_values = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N_counts = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    gamma = 1.0  # discount factor\n",
    "    total_steps = []  # List to store the number of steps taken in each episode\n",
    "\n",
    "    # Iterate over episodes\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()  # Initialize the environment and get the initial state\n",
    "        done = False\n",
    "        steps = 0  # Counter to track the number of steps taken in the episode\n",
    "\n",
    "        # Generate an episode using Ɛ-greedy policy\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # Choose a random action\n",
    "            else:\n",
    "                action = np.argmax(Q_values[state])  # Choose the action with maximum Q value\n",
    "            next_state, reward, done, info = env.step(action)  # Take the chosen action\n",
    "            N_counts[state][action] += 1  # Increment the visit count for the state-action pair\n",
    "            Q_values[state][action] += (reward + gamma * np.max(Q_values[next_state]) - Q_values[state][action]) / N_counts[state][action]  # Update Q value\n",
    "            state = next_state  # Update the current state\n",
    "            steps += 1  # Increment the step counter\n",
    "\n",
    "        total_steps.append(steps)  # Record the total number of steps taken in the episode\n",
    "\n",
    "    # Derive optimal policy from Q values\n",
    "    optimal_policy = np.argmax(Q_values, axis=1)\n",
    "\n",
    "    return optimal_policy, Q_values, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMrHpUmwnCB_",
    "outputId": "84de4079-f306-4219-c18b-9f9741676266"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "monte_carlo_es_policy, monte_carlo_es_Q_values, total_steps_es = monte_carlo_exploring_starts(env)\n",
    "on_policy_mc_control_policy, on_policy_mc_control_Q_values, total_steps_control = on_policy_monte_carlo_control(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDM3-jQknKO4",
    "outputId": "06f0e693-9987-4142-a437-913777960fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Steps taken to reach Optimal Policy using Monte Carlo ES: 3157424\n",
      "Total Number of Steps taken to reach Optimal Policy using On-Policy First-Visit MC Control: 17375\n"
     ]
    }
   ],
   "source": [
    "print(str.format('Total Number of Steps taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)))\n",
    "print(str.format('Total Number of Steps taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq2qrN1WnuM3",
    "outputId": "d095fff2-347c-48e1-ef0b-d0121047a788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Steps per Episode taken to reach Optimal Policy using Monte Carlo ES: 6314.848\n",
      "Average Number of Steps per Episode taken to reach Optimal Policy using On-Policy First-Visit MC Control: 34.75\n"
     ]
    }
   ],
   "source": [
    "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)/len(total_steps_es)))\n",
    "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)/len(total_steps_control)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rmWRUXBnx1B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
