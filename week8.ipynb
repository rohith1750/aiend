{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0cQ7nN0mVTc"
   },
   "source": [
    "Saai Narayann Maddu\n",
    "\n",
    "210968036\n",
    "\n",
    "Section - A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-xyrNX-kd8B",
    "outputId": "5f8bc5bd-aa0f-4148-a524-da3ce3c92116"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDf8Dh_CkhwI",
    "outputId": "fe6c0f47-9a40-4255-9236-a11cbb114091"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNgqAu6fknQK",
    "outputId": "6c171a80-12d6-4a0e-b752-28fa1380374a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def calculate_state_value(environment, current_state, value_matrix, discount_factor):\n",
    "    \"\"\"\n",
    "    Calculates the state values for a given state using the Bellman equation.\n",
    "\n",
    "    Args:\n",
    "    - environment: The environment object representing the Markov Decision Process (MDP).\n",
    "    - current_state: The current state for which the state value is to be calculated.\n",
    "    - value_matrix: A matrix containing the current estimated values for each state.\n",
    "    - discount_factor: Discount factor for future rewards.\n",
    "\n",
    "    Returns:\n",
    "    - action_values: An array containing the calculated value for each action in the current state.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of possible actions in the environment\n",
    "    num_actions = environment.action_space.n\n",
    "\n",
    "    # Initialize an array to store the value of each action\n",
    "    action_values = np.zeros(shape=num_actions)\n",
    "\n",
    "    # Iterate over each action\n",
    "    for action in range(num_actions):\n",
    "        # Iterate over each possible transition from the current state for the given action\n",
    "        for transition_prob, next_state, reward, done in environment.env.P[current_state][action]:\n",
    "            # Calculate the expected value for the action using the Bellman equation\n",
    "            action_values[action] += transition_prob * (reward + discount_factor * value_matrix[next_state])\n",
    "\n",
    "    return action_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "grZzXvjpk6d5"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, convergence_threshold=1e-9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Performs policy evaluation to estimate the state values for a given policy.\n",
    "\n",
    "    Args:\n",
    "    - policy: The policy matrix specifying the probability of taking each action in each state.\n",
    "    - environment: The environment object representing the Markov Decision Process (MDP).\n",
    "    - discount_factor: Discount factor for future rewards.\n",
    "    - convergence_threshold: Threshold for determining convergence in state value estimation.\n",
    "    - max_iterations: Maximum number of iterations for policy evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - state_values: An array containing the estimated value for each state under the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = environment.observation_space.n\n",
    "\n",
    "    # Initialize state values to zeros\n",
    "    state_values = np.zeros(shape=num_states)\n",
    "\n",
    "    # Counter for iterations\n",
    "    evaluation_iterations = 1\n",
    "\n",
    "    # Iterate until convergence or maximum iterations reached\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0  # Initialize the maximum change in value for this iteration\n",
    "        # Iterate over all states\n",
    "        for current_state in range(num_states):\n",
    "            new_state_value = 0  # Initialize the new value for the current state\n",
    "            # Iterate over all possible actions in the current state\n",
    "            for action, action_probability in enumerate(policy[current_state]):\n",
    "                # Iterate over all possible next states and their probabilities\n",
    "                for state_probability, next_state, reward, done in environment.P[current_state][action]:\n",
    "                    # Update the new state value based on the action, next state, and reward\n",
    "                    new_state_value += action_probability * state_probability * (reward + discount_factor * state_values[next_state])\n",
    "            # Update the maximum change in value\n",
    "            delta = max(delta, np.abs(state_values[current_state] - new_state_value))\n",
    "            # Update the state value with the new value\n",
    "            state_values[current_state] = new_state_value\n",
    "\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'Policy evaluation terminated after {evaluation_iterations} iterations.\\n')\n",
    "            return state_values\n",
    "\n",
    "    # If max_iterations reached without convergence, return the current state values\n",
    "    print(f'Policy evaluation reached maximum iterations ({max_iterations}).')\n",
    "    return state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gSd2_4-GlQtP"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Implements the policy iteration algorithm to find the optimal policy for the given environment.\n",
    "\n",
    "    Args:\n",
    "    - environment: The environment object representing the Markov Decision Process (MDP).\n",
    "    - discount_factor: Discount factor for future rewards.\n",
    "    - max_iterations: Maximum number of iterations for policy iteration.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_policy: The optimal policy matrix.\n",
    "    - optimal_value_function: The value function corresponding to the optimal policy.\n",
    "    \"\"\"\n",
    "\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "\n",
    "    # Initialize a uniform random policy matrix\n",
    "    policy_matrix = np.ones(shape=[num_states, num_actions]) / num_actions\n",
    "\n",
    "    # Counter for evaluated policies\n",
    "    evaluated_policies_count = 1\n",
    "\n",
    "    # Iterate until convergence or maximum iterations reached\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        stable_policy = False\n",
    "\n",
    "        # Evaluate the current policy\n",
    "        value_function = policy_evaluation(policy_matrix, environment, discount_factor)\n",
    "\n",
    "        # Improve the policy\n",
    "        for current_state in range(num_states):\n",
    "            current_action = np.argmax(policy_matrix[current_state])\n",
    "            action_values = calculate_state_value(environment, current_state, value_function, discount_factor)\n",
    "            best_action = np.argmax(action_values)\n",
    "\n",
    "            # Check if the policy is stable\n",
    "            if current_action != best_action:\n",
    "                stable_policy = True\n",
    "\n",
    "            # Update the policy matrix to select the best action in each state\n",
    "            policy_matrix[current_state] = np.eye(num_actions)[best_action]\n",
    "\n",
    "        evaluated_policies_count += 1\n",
    "\n",
    "        # If the policy is stable, return the optimal policy and value function\n",
    "        if not stable_policy:\n",
    "            print(f'Found a stable policy after {evaluated_policies_count:,} evaluations.\\n')\n",
    "            return policy_matrix, value_function\n",
    "\n",
    "    # If maximum iterations reached without convergence, return the current policy and value function\n",
    "    print(f'Maximum iterations reached ({max_iterations}). Returning current policy.\\n')\n",
    "    return policy_matrix, value_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "brfUvsyzlcDi"
   },
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1e-1, convergence_threshold=1e-9, max_iterations=1e4):\n",
    "    \"\"\"\n",
    "    Implements the value iteration algorithm to find the optimal policy for the given environment.\n",
    "\n",
    "    Args:\n",
    "    - environment: The environment object representing the Markov Decision Process (MDP).\n",
    "    - discount_factor: Discount factor for future rewards.\n",
    "    - convergence_threshold: Threshold for determining convergence in value iteration.\n",
    "    - max_iterations: Maximum number of iterations for value iteration.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_policy: The optimal policy matrix.\n",
    "    - optimal_state_values: The value function corresponding to the optimal policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize state values to zeros\n",
    "    state_values = np.zeros(environment.observation_space.n)\n",
    "\n",
    "    # Iterate until convergence or maximum iterations reached\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0  # Initialize the maximum change in value for this iteration\n",
    "\n",
    "        # Update state values\n",
    "        for current_state in range(environment.observation_space.n):\n",
    "            action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "            best_action_value = np.max(action_values)\n",
    "            delta = max(delta, np.abs(state_values[current_state] - best_action_value))\n",
    "            state_values[current_state] = best_action_value\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'\\nValue iteration converged at iteration #{iteration+1:,}')\n",
    "            break\n",
    "\n",
    "    # Create the optimal policy matrix based on the computed state values\n",
    "    optimal_policy = np.zeros(shape=[environment.observation_space.n, environment.action_space.n])\n",
    "\n",
    "    # Determine the best action for each state based on the computed state values\n",
    "    for current_state in range(environment.observation_space.n):\n",
    "        action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "        best_action = np.argmax(action_values)\n",
    "        optimal_policy[current_state, best_action] = 1.0\n",
    "\n",
    "    return optimal_policy, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eMhed8N4loU8"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy_performance(env, num_episodes, policy, max_actions=100, render=False):\n",
    "    \"\"\"\n",
    "    Plays episodes using the given policy and evaluates its performance.\n",
    "\n",
    "    Args:\n",
    "    - env: The environment object representing the Markov Decision Process (MDP).\n",
    "    - num_episodes: Number of episodes to play.\n",
    "    - policy: The policy matrix specifying the probability of taking each action in each state.\n",
    "    - max_actions: Maximum number of actions allowed per episode.\n",
    "    - render: Whether to render the environment.\n",
    "\n",
    "    Returns:\n",
    "    - total_wins: Total number of episodes won.\n",
    "    - total_rewards: Total rewards obtained across all episodes.\n",
    "    - average_reward: Average reward per episode.\n",
    "    - average_actions: Average number of actions taken per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    total_wins = 0\n",
    "    total_rewards, total_actions = 0, 0\n",
    "\n",
    "    # Play episodes\n",
    "    for episode in range(num_episodes):\n",
    "        current_state = env.reset()\n",
    "        episode_done, actions_taken = False, 0\n",
    "\n",
    "        # Play until the episode is done or maximum actions are taken\n",
    "        while actions_taken < max_actions:\n",
    "            # Choose action based on the policy\n",
    "            selected_action = np.argmax(policy[current_state])\n",
    "            next_state, reward, episode_done, _ = env.step(selected_action)\n",
    "\n",
    "            # Render the environment if required\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Update counters\n",
    "            actions_taken += 1\n",
    "            total_rewards += reward\n",
    "            current_state = next_state\n",
    "\n",
    "            # Break if episode is done\n",
    "            if episode_done:\n",
    "                total_wins += 1\n",
    "                break\n",
    "\n",
    "        total_actions += actions_taken\n",
    "\n",
    "    # Print total rewards and max actions taken in the last episode\n",
    "    print(f'Total rewards: {total_rewards:,}\\tMax actions: {actions_taken:,}')\n",
    "\n",
    "    # Calculate average reward and average actions per episode\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    average_actions = total_actions / num_episodes\n",
    "\n",
    "    print('')\n",
    "\n",
    "    return total_wins, total_rewards, average_reward, average_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9IFzHd4blzAM"
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "def evaluate_policies_and_agents(env):\n",
    "    \"\"\"\n",
    "    Evaluates different policies and agents using policy iteration and value iteration algorithms.\n",
    "\n",
    "    Args:\n",
    "    - env: The environment object representing the Markov Decision Process (MDP).\n",
    "\n",
    "    Returns:\n",
    "    - total_rewards_list: A list containing the total rewards obtained for each evaluated policy/agent.\n",
    "    \"\"\"\n",
    "\n",
    "    total_rewards_list = []\n",
    "\n",
    "    action_mapping = {\n",
    "        0: '\\u2191',  # up\n",
    "        1: '\\u2192',  # right\n",
    "        2: '\\u2193',  # down\n",
    "        3: '\\u2190'   # left\n",
    "    }\n",
    "\n",
    "    iteration_methods = [\n",
    "        ('Policy Iteration', policy_iteration),\n",
    "        ('Value Iteration', value_iteration)\n",
    "    ]\n",
    "\n",
    "    for method_name, method_func in iteration_methods:\n",
    "        policy_matrix, value_function = method_func(env)\n",
    "\n",
    "        print(f'Final policy using {method_name}:')\n",
    "        print(' '.join([action_mapping[action] for action in np.argmax(policy_matrix, axis=1)]))\n",
    "\n",
    "        total_wins, total_rewards, avg_reward, avg_actions = evaluate_policy_performance(env, num_episodes, policy_matrix)\n",
    "        total_rewards_list.append(total_rewards)\n",
    "\n",
    "        print(f'Number of wins = {total_wins:,}')\n",
    "        print(f'Average reward = {avg_reward:.2f}')\n",
    "        print(f'Average actions = {avg_actions:.2f}')\n",
    "\n",
    "    return total_rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKg55X6emFNY",
    "outputId": "1cd2dc32-e631-4744-a3de-b332a23b2982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation terminated after 66 iterations.\n",
      "\n",
      "Policy evaluation terminated after 417 iterations.\n",
      "\n",
      "Policy evaluation terminated after 527 iterations.\n",
      "\n",
      "Found a stable policy after 4 evaluations.\n",
      "\n",
      "Final policy using Policy Iteration:\n",
      "↑ ← ← ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards: 722.0\tMax actions: 47\n",
      "\n",
      "Number of wins = 1,000\n",
      "Average reward = 0.72\n",
      "Average actions = 45.84\n",
      "\n",
      "Value iteration converged at iteration #8\n",
      "Final policy using Value Iteration:\n",
      "→ ← ↓ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n",
      "Total rewards: 446.0\tMax actions: 19\n",
      "\n",
      "Number of wins = 1,000\n",
      "Average reward = 0.45\n",
      "Average actions = 26.97\n"
     ]
    }
   ],
   "source": [
    "rewards = evaluate_policies_and_agents(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9zQe-tLmHbo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
